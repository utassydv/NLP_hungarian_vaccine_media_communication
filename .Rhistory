library(dplyr)
library(Hmisc)
library(reshape2)
library(tidyverse)
library(tidytext)
library(lubridate)
library(ggplot2)
library(viridis)
library(stringr)
library(igraph)
library(ggraph)
library(topicmodels)
library(tidyr)
library(widyr)
# data preparation ------------------------------------------------------------
articles_telex <- readRDS('data/raw/articles_telex.rds')
articles_24hu <- readRDS('data/raw/articles_24hu.rds')
articles_index<- readRDS('data/raw/articles_index.rds')
articles_origo<- readRDS('data/raw/articles_origo.rds')
# some cleaning
articles_origo <- articles_origo %>% select(title=headline,
url,
date_time=published,
content,
site)
# get date from url with stringr
# date_pattern <- "[0-9]{4}/[0-9]{2}/[0-9]{2}"
# articles_444hu$date_time <- str_extract(articles_444hu$url, date_pattern)
# cleaning date_time columns with lubridate
articles_telex$date <- as.Date(ymd_hm(articles_telex$date_time))
articles_24hu$date <- as.Date(ymd_hm(articles_24hu$date_time))
articles_index$date <- as.Date(ymd_hms(articles_index$date_time))
articles_origo$date <- as.Date(ymd_hms(articles_origo$date_time))
# combine all the news sites:
all_sites <- rbind(articles_telex,articles_24hu, articles_index, articles_origo)
# filtering for covid-19 times > 2020
all_sites <- all_sites %>% filter(date >= as.Date('2020-01-01'))
# concatenating paragraphs withing articles
all_sites$content_full <- mapply(paste, all_sites$content, sep = ' ', collapse =' ')
tidy_all_sites <- all_sites %>%
unnest_tokens(word, content_full) %>%
filter(!str_detect(word, "\\d+"))
# removing stop words
hu_stop_word <- read_csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-hu/master/stopwords-hu.txt", col_names = FALSE)
colnames(hu_stop_word) <- "word"
tidy_all_sites <- tidy_all_sites %>% anti_join(hu_stop_word)
positive_words <- read_csv("data/PrecoSenti/PrecoPos.txt", col_names = FALSE) %>%
mutate(sentiment=1)
negative_words <- read_csv("data/PrecoSenti/PrecoNeg.txt", col_names = FALSE) %>%
mutate(sentiment=-1)
hungarian_sentiment <- rbind(positive_words, negative_words)
colnames(hungarian_sentiment) <- c('word', 'sentiment')
tidy_all_sites_sentiment <- tidy_all_sites %>% inner_join(hungarian_sentiment)
# positive negative issue:
hungarian_sentiment <- hungarian_sentiment %>%
filter(!(word %in% c('pozitív','negatív')))
tidy_all_sites_sentiment <- tidy_all_sites %>% inner_join(hungarian_sentiment)
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment))
View(weekly_sentiment_score)
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment)) %>%
mutate( date = ymd(year,weeks(week)))
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment)) %>%
mutate( date = ymd(paste0(year,'-',weeks(week))))
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment))
weekly_sentiment_score$date <- ymd(paste0(year,'-',weeks(week)))
weekly_sentiment_score$date <- ymd(str_c(year,'-',weeks(week)))
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment)) %>%
mutate( date = year +weeks(week))
View(weekly_sentiment_score)
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment)) %>%
mutate( date = ymd(year +weeks(week)))
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment)) %>%
mutate( date = make_date(year +weeks(week)))
View(weekly_sentiment_score)
weekly_sentiment_score&date <- make_date(weekly_sentiment_score$year + weeks(weekly_sentiment_score$week))
weekly_sentiment_score$date <- make_date(weekly_sentiment_score$year + weeks(weekly_sentiment_score$week))
View(weekly_sentiment_score)
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date)) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment)) %>%
mutate( date = make_date(year) + weeks(week))
View(weekly_sentiment_score)
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(bin=5)+
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col()+
facet_wrap(~site, ncol = 1, scales = "free_y")
View(weekly_sentiment_score)
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm)) +
geom_col(bin=5)
#aggregated time series sentiment by site
weekly_sentiment_score <- tidy_all_sites_sentiment %>%
group_by(year = year(date),
week = week(date),
site) %>%
summarise( daily_sentiment_norm = sum(sentiment)/n(),
n=n(),
daily_sentiment= sum(sentiment)) %>%
mutate( date = make_date(year) + weeks(week))
ggplot(daily_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col()+
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col()+
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
geom_smooth()
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
geom_smooth() %>%
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
geom_smooth() +
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
geom_smooth(n=10) +
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
geom_smooth(n=5) +
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
geom_smooth(n=2) +
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE)+
geom_smooth(n=1000) +
facet_wrap(~site, ncol = 1, scales = "free_y")
ggplot(weekly_sentiment_score, aes(x=date,y=daily_sentiment_norm, fill = site)) +
geom_col(show.legend = FALSE) +
facet_wrap(~site, ncol = 1, scales = "free_y")
not_words_contribution
library("ggpubr")
tidy_all_sites_by_title <- all_sites %>%
filter(site=='telex')
unnest_tokens(word, title, drop=F) %>%
filter(!str_detect(word, "\\d+")) %>%
anti_join(hu_stop_word) %>%
count(title, word)
coocurring_words_title <- tidy_all_sites_by_title %>% pairwise_count(word, title, sort = TRUE, upper = FALSE)
saveRDS(coocurring_words_title, file = 'data/raw/coocurring_words_title_telex.rds')
set.seed(1234)
coocurring_words_content_plot_title_telex <- coocurring_words_title %>%
filter(n >= 40) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()+
ggtitle("Co-occuring words in article titles (telex.hu)")
coocurring_words_content_plot_title_telex
ggsave("plots/coocurring_words_content_plot_title_telex.png", plot = coocurring_words_content_plot_title_telex)
